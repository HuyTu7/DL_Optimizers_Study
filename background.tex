\subsection*{Related Work}
Training a large deep neural network is very computationally expensive. You et al. (2017) proposed a new training algorithm LARS to solve this problem. Using the LARS algorithm, the Alexnet can be trained with a batch size of 8k and Resnet-50 can be trained with a batch size of 32k without loss in accuracy.

You et al. (2019) developed a new optimization algorithm LAMB especially for large batch size training. This algorithm uses a layer-wise adaptation strategy to cater to large batch size problem. Training the BERT language model with LAMB optimizier can drastically reduce the training time from 3 days to 76 minutes without the reduction of performance.