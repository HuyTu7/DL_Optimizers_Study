Deep neural networks typically have a large upfront cost associated with computational power and training dataset size. Even with modern training optimization algorithms such as Stochastic Gradient Descent~\cite{SGD}, training cost poses the biggest obstacle to achieving higher test accuracy. The advent GPU-based parallel processing has contributed the most to surpassing this obstacle, allowing previously impractical networks to become possible. For instance, training state-of-the-art NN's like BERT~\cite{bert} and ResNet-50~\cite{resnet} were able to be done in 3 days and 29 hours, respectively, as opposed to the original training time of several weeks. This feat was not possible even with the largest scale server processors before the beginning of this decade. 

Recent research in lowering training cost has been focused on optimizing the actual mathematical calculations during the process. Methods to make minor improvements include choosing optimal hyperparameters like the learning rate, automated perceptron pruning/dropout\cite{autopercept}, and by applying some form of gradient normalization. More complicated adaptive approaches attempt to better understand the direction of negative gradient to achieve more effective steps, typically using moving averages of the first and second moments. Popular optimizers like Adam and AdamW incorporate this adaptive approach and have shown substantial improvements over its predecessors\cite{Loshchilov2017FixingWD}. More advanced algorithms such as Layer-wise Adaptive Moments opti-mizer for Batch training (LAMB~\cite{You2020Large}) and Layer-wise Adaptive
Rate Scaling (LARS~\cite{qian2020impact, ginsburg2018large}) take this mentality several steps further and apply layer-wise adaptive gradient pre/normalization. You et al. showed that these approaches improve the initial loss decrease and allow for larger batch-size and distributed training while achieving comparable results (e.g. reducing BERT training time from three days to 76 minutes).  

However, the Machine Learning practitioners have been skeptical of these improvements and have not investigated or applied these layer-wise optimizers in their domains. This paper shall investigate, compare, and implement these innovative optimization techniques in relation to deep NN's in the Natural Language Processing (NLP), Computer Vision (CV) and Speech Processing domains. Specifically, the scope of the paper is focusing on the medium and smaller scale of the models (with median of 13 million parameters) in comparison to state of the art paper that testing the optimizers effectiveness on BERT ($\sim$110 million parameters). 


\begin{table}[t]
\centering
\caption{Overview of the studies: types of the explored tasks with corresponding models and optimizers}
\label{tab:LargeBatch}
\begin{tabular}{ccc} 
 \toprule
\textbf{Domain Task} & \textbf{Models} & \textbf{\# of Parameters} \\
 \midrule
  Image Classification & CNN \cite{CNN} & $\sim$440,000 \\
  QA System & DrQA \cite{DrQA} & $\sim$13,000,000 \\ 
  Speech to Text System & Deep Speech 2 \cite{deepspeech} &  $\sim$15,000,000\\
  \bottomrule
\end{tabular}

\vspace{10pt}

\begin{tabular}{cccc} 
 \toprule
 &  \multicolumn{2}{c}{\textbf{Optimizers}} & \\
 \midrule
 SGD \cite{SGD} & LARS \cite{ginsburg2018large} & Adam \cite{adam}  &  LAMB \cite{You2020Large} \\
  \bottomrule
\end{tabular}
\end{table}

The study cases include: 

\begin{itemize}
    \item Image classification is a process of labeling and categorizing pictures. For most image classification tasks, convolutional neural networks are the most effective at achieving high accuracy with a moderate number of perceptron connections. However, with recurrent convolutional and pooling layers, these networks can become very deep (large number of parameters) relatively quickly\cite{CNN}. Many issues arise with standard back-propagation techniques of these deep CNN's, putting a large burden on the effectiveness of the optimizer chosen to train the model. Even after thousands of iterations and epochs, some optimizers still fail converge to optimal weights\cite{Sun2019OptimizationFD} with used on models with high numbers of layers. 
    % We expect that the layer-wise approach taken by the LAMB or LARS optimizers will thus improve training time of these types deep networks significantly, as they should be able to better incorporate information from the individual layer gradients. This decrease in training time should be seen as a faster decrease in the loss function value while still maintaining relatively similar accuracy on the test/validation set.
    
    \item Another common application of machine learning is the use of NN's for question answering (QA) system, which belongs to the category of information retrieval. These systems consist of three parts: question classification, information retrieval, and answer extraction. Since these models  must be able to respond with a large vocabulary and are very dynamic in nature, they usually suffer from extremely large training times. The overall architecture of the model we will test will include a LSTM network with attention, which is the standard model used in production. 
    
    %We believe that the application of layer-wise adaptive scaling and pre-normalization will improve the initial loss decrease and allow for larger batch-size/distributed training.\\
    \item The final type of machine learning model tested in this paper will be a speech to text system. This involves transforming raw audio signals into human-readable text. Consumer products such as the Amazon Alexa, Siri from Apple, the Google Assistant all utilize the most cutting-edge speech-to-text models. These models are usually multimodal including acoustic, pronunciation, and language models. Each part can utilize a deep NN to train separately and that makes speech recognition algorithms very computationally expensive and incorporate extremely large numbers of trainable parameters. 
    
    %We expect that optimizer's like LAMB/LARS could help reduce this computational cost of training when used on modern speech-to-text architecture such as the LAS model\cite{LAS}.

\end{itemize}

According to You et al. \cite{ginsburg2018large, You2020Large} work, we also expect that the layer-wise approach, LAMB or LARS optimizers, will thus improve training time of deep networks for these application domains by adaptively learn the initial losses. It can better incorporate information from the individual layer gradients. This decrease in training time should be seen as a faster decrease in the loss function value while still maintaining relatively similar performance.


In summary, the contributions of our study include:
\begin{enumerate}
    \item Validate the previous work results that this paper based on \cite{ginsburg2018large,You2020Large}.
    \item Explore more study cases (NLP, CV, and Speech) and model sizes (median of 13 million parameters than the original work did (only ResNet50 \cite{resnet} and BERT \cite{bert}). 
    \item Testing the effectiveness of layer-wise optimization techniques on smaller batch sizes training (8 as the smallest for Speech and 64 for NLP)  as the You et al. tested on large batch sizes of {4k and >8k}.
\end{enumerate}




%but are based on some forms of a Hidden Markov Model (HMM) \cite{HMM}. Preprocessing of the input data is usually done by dividing the speech signal into segmented chunks and applying signal processing transforms (i.e. Fast Fourier Transform)\cite{FFT}. This makes speech recognition algorithms very computationally expensive and incorporate extremely large numbers of trainable parameters. We expect that optimizer's like LAMB/LARS could help reduce this computational cost of training when used on modern speech-to-text architecture such as the LAS model\cite{LAS}.







% \begin{table}[!t]
% \vspace{-15pt}
% \scriptsize
% \vspace{7pt}
% \caption{Plan of project work.}\label{tbl:planofwork}
% \vspace{-15pt}
% \begin{center}
% \begin{tabular}{ l|c|l}
%   \multicolumn{1}{c|}{Standard} &
%  \multicolumn{1}{c|}{Date} &
%  \multicolumn{1}{c}{Responsibilities}\\
%  %& & \\
% \hline
% & & We will research necessary papers and  \\
%  Literature Review & 9/07 - 9/25 &  contribute information to address how the  \\
%  & &  problem relates to the group's interests. \\\hline
% & & Each of us will propose appropriate \\
% Experiment Setup & 	09/25 - 10/02 & application, setting, and dataset for this\\
% & &  project's direction.\\\hline
% & & We will delegate coding parts based off of \\
% Implementation & 10/02 - 10/16  & each individual's strength (e.g. each optimizer \\
% & & per individual group member) \\\hline
% & & We will share the exploration of applicable \\
% Mathematical and & 10/16 - 10/30 &  theories (e.g. LARS \& LAMB) while \\ 
% Result Analysis & &	  evaluate the results deeply in regards to  \\
% & & the problem's context\\\hline
% & & Each of us will help report background \\
% Documentation & 10/30 - 11/13 &  theories, outcomes, discussion, and possible\\
% & &  future directions. \\


% \end{tabular}
% \end{center}
% \vspace{-15pt}
% \end{table}