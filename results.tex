
\subsection{Image Classification}
% lf 0.01

% Sgd & 0.20985 & 0.21054 & 0.23648 & 0.27958 & \\
% Lars & 0.32058 & 0.39033 & 0.54637 & 0.86530 & \\
% Adam & 3.25820 & 3.25817 & 3.25811 & 0.41513 & \\
% Lamb & 0.18673 & 0.18932 & 0.19189 & 0.19151 & \\

% 0.001
% Sgd & 0.30633 & 0.41941 & 0.65746 & 1.02366 & \\
% Lars & 1.24437 & 1.85403 & 2.69716 & 3.04892 & \\
% Adam & 0.18780 & 0.19604 & 0.19640 & 0.19186 & \\
% Lamb & 0.19837 & 0.20102 & 0.20386 & 0.20411 & \\

\begin{table}[!b]
\vspace{-5pt}
\small
\vspace{7pt}
\caption{Image classification performance based on optimizers (SGD, Adam, LARS, LAMB) across different batch sizes (1024, 2048, 4096, 8192). Highlighted gray cells indicate best performing optimizer. The learning rate of the top and bottom table are 0.01 and 0.001 respectively.}\label{tbl:img_results}
\vspace{-10pt}
\begin{center}
\begin{tabular}{ c|c|c|c|c}
\multicolumn{1}{c|}{} &  \multicolumn{4}{c}{Batch Size}\\ \cline{2-5}
\multicolumn{1}{c|}{Optimizer} &
 \multicolumn{1}{c|}{1024} &
 \multicolumn{1}{c|}{2048} &
 \multicolumn{1}{c|}{4096} &
 \multicolumn{1}{c}{8192} \\
 \hline
SGD & 0.21 & 0.21 & 0.24 & 0.28 \\
LARS & 0.32 & 0.39 & 0.55 & 0.87 \\
Adam & 3.26 & 3.26 & 3.26 & 0.42 \\
LAMB & \cellcolor{gray!30} 0.19 & \cellcolor{gray!30} 0.19 & \cellcolor{gray!30} 0.19 & \cellcolor{gray!30}  0.19 \\

\end{tabular}

\vspace{10pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tabular}{ c|c|c|c|c}
\multicolumn{1}{c|}{} &  \multicolumn{4}{c}{Batch Size}\\ \cline{2-5}
\multicolumn{1}{c|}{Optimizer} &
 \multicolumn{1}{c|}{1024} &
 \multicolumn{1}{c|}{2048} &
 \multicolumn{1}{c|}{4096} &
 \multicolumn{1}{c}{8192} \\ \hline
SGD & 0.31 & 0.42 & 0.66 & 1.02 \\
LARS & 1.24 & 1.85 & 2.70 & 3.05 \\
Adam & \cellcolor{gray!30} 0.19 & \cellcolor{gray!30} 0.20 & \cellcolor{gray!30} 0.20 & \cellcolor{gray!30} 0.19 \\
LAMB & 0.20 & \cellcolor{gray!30} 0.20 & \cellcolor{gray!30} 0.20 & 0.20 \\

\end{tabular}
\vspace{-15pt}
\end{center}
\end{table}



The result of image classification is shown in Table \ref{tbl:img_results}. We researched four optimizers (SGD, Adam, LARS, LAMB) across four different batch sizes (1024, 2048, 4096, 8192) with two different learning rate (0.01, 0.001). The grey cells represent the winner (the one with the lowest loss in each column). The results obtained are reasonable since LAMB outperformed all other optimizers under the conditions of a higher learning rate (0.01) and was unaffected by batch size. LAMB's performance was nearly as good as Adam in the lower learning rate case (0.001). This difference could be accounted to for the extra normalization (and thus most likely smaller magnitude) weight updates. Adam performs the worst at the higher learning rates, which is probably because it was set so high that Adam failed to learn anything about the first or second moments from the initial time step. It shows that Adam is much more susceptible to changes in learning rate, but when performed across different batch sizes there was not too much of an overall difference seen from the table. In Figure \ref{fig:cv_plots}, it can be seen that although Adam achieved a similar global minimum of the loss function at some point across batch sizes, its performance (in terms of number of epochs to reach said minimum) was slightly hindered. Thus, Adam performed best lower batch sizes, which confirms the previous hypothesis. 

Surprisingly, LARS was the worst performer in almost all cases. SGD bested LARS in every situation as well. Comparing performance across different learning rates, it can be seen that a higher learning rate helped LARS come much closer to a local minima. This is most likely caused by the same principal in the comparison of LAMB to Adam, where the extra normalization decreased the overall magnitude of the update step. The performance of LARS matches the findings seen in all other case studies. For both SGD and LARS, it can be shown that overall performance is indirectly related to batch size as when batch size increased, the performance became much worse. However, for the LAMB optimizer the performance stayed the same, which indicates that LAMB might be useful for large batch training.

\subsection{QA System}

\begin{table}[!b]
\vspace{-5pt}
\small
\vspace{7pt}
\caption{QA System performance based on optimizers (SGD, Adam, LARS, LAMB) across different batch sizes (64, 128, 256, 512). Highlighted gray cells indicate best performing optimizer.}\label{tbl:qa_results}
\vspace{-10pt}
\begin{center}
\begin{tabular}{ c|c|c|c|c}
\multicolumn{1}{c|}{} &  \multicolumn{4}{c}{Batch Size}\\ \cline{2-5}
\multicolumn{1}{c|}{Optimizer} &
 \multicolumn{1}{c|}{64} &
 \multicolumn{1}{c|}{128} &
 \multicolumn{1}{c|}{256} &
 \multicolumn{1}{c}{512} \\
 %& & \\
\hline
SGD & 4.76 & 5.00 & 5.38 & 6.45\\
Adam &  4.59 & \cellcolor{gray!30} 4.59 &  \cellcolor{gray!30} 4.49 & \cellcolor{gray!30} 4.53\\
LARS & 8.36 & 8.42 & 8.57 & 9.43\\
LAMB & \cellcolor{gray!30} 4.58 & 4.62 & 4.59 & 4.86\\
\end{tabular}
\end{center}
\vspace{-15pt}
\end{table}

The experimentation is conducted by deploying the model from \S\ref{sec:method}.B against the dedicated four optimizers in this study (SGD, Adam, LARS, and LAMB) across various batch sizes (64, 128, 256, 512). The results are reported in the Table~\ref{tbl:qasystem}. The gray cells denote the winner (the one with the lowest loss overall test loss values in the same column) across each batch size. The results were surprising as LAMB only outperformed Adam in a batch size of 64 but lost in the other batch sizes. LARS performed the worst across every run, where it was bested by traditional SGD consistently. Moreover, as the batch size increased, the gap between performances increased. This contradicts what has been observed in current literature \cite{ginsburg2018large, }. We did not observe the performance increase of the QA system with larger batch sizes across the layer-wise adaptive techniques. To our surprise, the results are actually opposite to it where as the QA system with laywer-wise techniques work better for smaller batch sizes. 

\subsection{Speech-to-Text}

The experimentation is conducted by deploying the model from \S\ref{sec:method}.C against the dedicated four optimizers in this study (SGD, Adam, LARS, and LAMB) across various batch sizes (8, 16, 32). The results are reported in the Table~\ref{tbl:speech_results}. The gray cells denote the winner (the one with the lowest loss overall test loss values in the same column) across each batch size. The results are consistent with QA System results that Adam only outperformed LAMB at batch size of 16 but lost in the other two batch sizes (8 and 32). LARS performed the worst across every run, where it was bested by traditional SGD consistently. In some cases, LARS was unable to train the network at all, as indicated by the NaN* values. This was most likely due to exploding gradients during the normalization at the update steps. Moreover, as the batch size increased, the performances gap increased.

\begin{table}[!t]
\vspace{-5pt}
\small
\vspace{7pt}
\caption{Speech-to-Text performance based on optimizers (SGD, Adam, LARS, LAMB) across different batch sizes (8, 16, 32). Highlighted gray cells indicate best performing optimizer. The NaN* values indicate training was unable to converge.}\label{tbl:speech_results}
\vspace{-10pt}
\begin{center}
\begin{tabular}{ c|c|c|c}
 &  \multicolumn{3}{c}{Batch Size}\\ 
\cline{2-4}
\multicolumn{1}{c|}{Optimizer} &
 \multicolumn{1}{c|}{8} &
 \multicolumn{1}{c|}{16} &
 \multicolumn{1}{c}{32}  \\
\hline
SGD & 2.80 & 2.83 & 2.85 \\
LARS & NaN* & NaN* & 2.86 \\
Adam & \cellcolor{gray!30} 1.03 & 2.21 & \cellcolor{gray!30}  0.77 \\
LAMB & 1.06 & \cellcolor{gray!30} 0.89 & 1.17 \\
\end{tabular}
\end{center}
\vspace{-15pt}
\end{table}