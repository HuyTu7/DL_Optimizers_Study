\subsection{Image Classification}
% lf 0.01

% Sgd & 0.20985 & 0.21054 & 0.23648 & 0.27958 & \\
% Lars & 0.32058 & 0.39033 & 0.54637 & 0.86530 & \\
% Adam & 3.25820 & 3.25817 & 3.25811 & 0.41513 & \\
% Lamb & 0.18673 & 0.18932 & 0.19189 & 0.19151 & \\

% 0.001
% Sgd & 0.30633 & 0.41941 & 0.65746 & 1.02366 & \\
% Lars & 1.24437 & 1.85403 & 2.69716 & 3.04892 & \\
% Adam & 0.18780 & 0.19604 & 0.19640 & 0.19186 & \\
% Lamb & 0.19837 & 0.20102 & 0.20386 & 0.20411 & \\

\begin{table}[!t]
\vspace{-5pt}
\small
\vspace{7pt}
\caption{Image classification performance based on optimizers (SGD, Adam, LARS, LAMB) across different batch sizes (1024, 2048, 4096, 8192). Highlighted gray cells indicate best performing optimizer. The learning rate of the top and bottom table are 0.01 and 0.001 respectively.}\label{tbl:img_results}
\vspace{-10pt}
\begin{center}
\begin{tabular}{ c|c|c|c|c}
\multicolumn{1}{c|}{} &  \multicolumn{4}{c}{Batch Size}\\ \cline{2-5}
\multicolumn{1}{c|}{Optimizer} &
 \multicolumn{1}{c|}{1024} &
 \multicolumn{1}{c|}{2048} &
 \multicolumn{1}{c|}{4096} &
 \multicolumn{1}{c}{8192} \\
 \hline
SGD & 0.21 & 0.21 & 0.24 & 0.28 \\
LARS & 0.32 & 0.39 & 0.55 & 0.87 \\
Adam & 3.26 & 3.26 & 3.26 & 0.42 \\
LAMB & \cellcolor{gray!30} 0.19 & \cellcolor{gray!30} 0.19 & \cellcolor{gray!30} 0.19 & \cellcolor{gray!30}  0.19 \\

\end{tabular}

\vspace{10pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tabular}{ c|c|c|c|c}
\multicolumn{1}{c|}{} &  \multicolumn{4}{c}{Batch Size}\\ \cline{2-5}
\multicolumn{1}{c|}{Optimizer} &
 \multicolumn{1}{c|}{1024} &
 \multicolumn{1}{c|}{2048} &
 \multicolumn{1}{c|}{4096} &
 \multicolumn{1}{c}{8192} \\ \hline
SGD & 0.31 & 0.42 & 0.66 & 1.02 \\
LARS & 1.24 & 1.85 & 2.70 & 3.05 \\
Adam & \cellcolor{gray!30} 0.19 & \cellcolor{gray!30} 0.20 & \cellcolor{gray!30} 0.20 & \cellcolor{gray!30} 0.19 \\
LAMB & 0.20 & \cellcolor{gray!30} 0.20 & \cellcolor{gray!30} 0.20 & 0.20 \\

\end{tabular}
\vspace{-15pt}
\end{center}
\end{table}

The result of image classification is shown in Table \ref{t}. We researched four optimizers (SGD, Adam, LARS, LAMB) across four different batch sizes (1024, 2048, 4096, 8192) with two different learning rate (0.01, 0.001). 


\subsection{QA System}

\begin{table}[!t]
\vspace{-5pt}
\small
\vspace{7pt}
\caption{QA System performance based on optimizers (SGD, Adam, LARS, LAMB) across different batch sizes (64, 128, 256, 512). Highlighted gray cells indicate best performing optimizer.}\label{tbl:qa_results}
\vspace{-10pt}
\begin{center}
\begin{tabular}{ c|c|c|c|c}
\multicolumn{1}{c|}{} &  \multicolumn{4}{c}{Batch Size}\\ \cline{2-5}
\multicolumn{1}{c|}{Optimizer} &
 \multicolumn{1}{c|}{64} &
 \multicolumn{1}{c|}{128} &
 \multicolumn{1}{c|}{256} &
 \multicolumn{1}{c}{512} \\
 %& & \\
\hline
SGD & 4.76 & 5.00 & 5.38 & 6.45\\
Adam &  4.59 & \cellcolor{gray!30} 4.59 &  \cellcolor{gray!30} 4.49 & \cellcolor{gray!30} 4.53\\
LARS & 8.36 & 8.42 & 8.57 & 9.43\\
LAMB & \cellcolor{gray!30} 4.58 & 4.62 & 4.59 & 4.86\\
\end{tabular}
\end{center}
\vspace{-15pt}
\end{table}



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.7\linewidth]{img/drqa_128_blars_oadam_psgd_rlamb.png}
    \caption{LARS (light blue), SGD (pink), LAMB (red), and Adam (orange) test loss convergence graph.}
    \label{fig:128}
\end{figure}

The experimentation is conducted by deploying the model from \S\ref{sec:method}.B against the dedicated four optimizers in this study (SGD, Adam, LARS, and LAMB) across various batch sizes (64, 128, 256, 512). The results are reported in the Table~\ref{tbl:qasystem}. The gray cells denote the winner (the one with the lowest loss overall test loss values in the same column) across each batch size. The results were surprising as LAMB only outperformed Adam in a batch size of 64 but lost in the other batch sizes. LARS performed the worst across every run, where it was bested by traditional SGD consistently. Moreover, as the batch size increased, the gap between performances increased. This contradicts what has been observed in current literature \cite{ginsburg2018large}. We did not observe the performance increase of the QA system with larger batch sizes across the layer-wise adaptive techniques. However, there are two possible explanations for these results, which include: 
\begin{itemize}
    \item The batch sizes tested in the QA system were statistically small in comparison to most literature (which reported sizes of 4K or >8K)
    \item Based on Figure \ref{fig:256}, Adam over-fitted a lot more quickly and triggered the early stop procedure, while the other algorithms did not. A similar trend can be observed in Figure \ref{fig:128} where LAMB converged very slowly and it can easily be deduced that a longer training time would allow for an even lower test loss value (and higher overall accuracy) than ADAM ever could reach
\end{itemize}

\subsection{Speech-to-Text}
\begin{table}[!t]
\vspace{-5pt}
\small
\vspace{7pt}
\caption{Speech-to-Text performance based on optimizers (SGD, Adam, LARS, LAMB) across different batch sizes (8, 16, 32). Highlighted gray cells indicate best performing optimizer.}\label{tbl:speech_results}
\vspace{-10pt}
\begin{center}
\begin{tabular}{ c|c|c|c}
 &  \multicolumn{3}{c}{Batch Size}\\ 
\cline{2-4}
\multicolumn{1}{c|}{Optimizer} &
 \multicolumn{1}{c|}{8} &
 \multicolumn{1}{c|}{16} &
 \multicolumn{1}{c}{32}  \\
\hline
SGD & 2.80 & 2.83 & 2.85 \\
LARS & NaN* & NaN* & 2.86 \\
Adam & \cellcolor{gray!30} 1.03 & 2.21 & \cellcolor{gray!30}  0.77 \\
LAMB & 1.06 & \cellcolor{gray!30} 0.89 & 1.17 \\
\end{tabular}
\end{center}
\vspace{-15pt}
\end{table}